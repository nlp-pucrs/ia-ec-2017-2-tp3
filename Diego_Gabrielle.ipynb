{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#from ggplot import *\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA, RandomizedPCA\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99,\n",
    "                        top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carrega o Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "O dataset escolhido foi o MNIST, que é um conjunto de imagens de digitos de 0 a 9 escritos a mão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata(\"MNIST original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Utilização do PCA para o redimensionamento das imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizamos 56000 amostras para treinamento e 14000 amostras para teste\n",
      "numero de amostras total do dataset: 70000\n",
      "numero de features: 784\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data / 255., mnist.target, test_size=0.20)\n",
    "print('Utilizamos %d amostras para treinamento e %d amostras para teste' % (len(X_train), len(X_test)))\n",
    "\n",
    "n_samples = mnist.data.shape[0]\n",
    "n_features = mnist.data.shape[1]\n",
    "\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "print('numero de amostras total do dataset:', n_samples)\n",
    "print('numero de features:', n_features)\n",
    "\n",
    "n_components = 16\n",
    "\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized', whiten=True).fit(X_train)\n",
    "\n",
    "pca_numbers = pca.components_.reshape((n_components, 28, 28))\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhEAAAHkCAYAAACAKX0lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6cc5f56a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcanumber_titles= []\n",
    "for i in range(mnist.data.shape[0]):\n",
    "    pcanumber_titles.append(\"Digit \"+str(i))\n",
    " \n",
    "plot_gallery(pca_numbers, pcanumber_titles, 28, 28)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Treinamento utilizando o MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(89,), activation='tanh', max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "#activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.31032216\n",
      "Iteration 2, loss = 0.15207168\n",
      "Iteration 3, loss = 0.11059868\n",
      "Iteration 4, loss = 0.08686078\n",
      "Iteration 5, loss = 0.07143249\n",
      "Iteration 6, loss = 0.05938503\n",
      "Iteration 7, loss = 0.05122458\n",
      "Iteration 8, loss = 0.04306196\n",
      "Iteration 9, loss = 0.03705915\n",
      "Iteration 10, loss = 0.03180612\n",
      "Iteration 11, loss = 0.02753249\n",
      "Iteration 12, loss = 0.02376474\n",
      "Iteration 13, loss = 0.02064492\n",
      "Iteration 14, loss = 0.01803107\n",
      "Iteration 15, loss = 0.01579507\n",
      "Iteration 16, loss = 0.01362228\n",
      "Iteration 17, loss = 0.01212235\n",
      "Iteration 18, loss = 0.01064153\n",
      "Iteration 19, loss = 0.00954750\n",
      "Iteration 20, loss = 0.00848149\n",
      "Iteration 21, loss = 0.00770892\n",
      "Iteration 22, loss = 0.00692291\n",
      "Iteration 23, loss = 0.00626042\n",
      "Iteration 24, loss = 0.00581721\n",
      "Iteration 25, loss = 0.00531396\n",
      "Iteration 26, loss = 0.00491804\n",
      "Iteration 27, loss = 0.00458133\n",
      "Iteration 28, loss = 0.00436730\n",
      "Iteration 29, loss = 0.00410546\n",
      "Iteration 30, loss = 0.00386134\n",
      "Iteration 31, loss = 0.00367231\n",
      "Iteration 32, loss = 0.00349656\n",
      "Iteration 33, loss = 0.00329620\n",
      "Iteration 34, loss = 0.00314805\n",
      "Iteration 35, loss = 0.00304891\n",
      "Iteration 36, loss = 0.00291688\n",
      "Iteration 37, loss = 0.00280382\n",
      "Iteration 38, loss = 0.00273195\n",
      "Iteration 39, loss = 0.00262080\n",
      "Iteration 40, loss = 0.00253310\n",
      "Iteration 41, loss = 0.00243385\n",
      "Iteration 42, loss = 0.00236545\n",
      "Iteration 43, loss = 0.00231018\n",
      "Iteration 44, loss = 0.00224129\n",
      "Iteration 45, loss = 0.00216953\n",
      "Iteration 46, loss = 0.00212240\n",
      "Iteration 47, loss = 0.00205912\n",
      "Iteration 48, loss = 0.00202345\n",
      "Iteration 49, loss = 0.00197041\n",
      "Iteration 50, loss = 0.00192334\n",
      "Iteration 51, loss = 0.00187594\n",
      "Iteration 52, loss = 0.00184274\n",
      "Iteration 53, loss = 0.00180140\n",
      "Iteration 54, loss = 0.00176634\n",
      "Iteration 55, loss = 0.00172965\n",
      "Iteration 56, loss = 0.00169876\n",
      "Iteration 57, loss = 0.00166959\n",
      "Iteration 58, loss = 0.00163914\n",
      "Iteration 59, loss = 0.00160526\n",
      "Iteration 60, loss = 0.00158054\n",
      "Iteration 61, loss = 0.00155483\n",
      "Iteration 62, loss = 0.00152842\n",
      "Iteration 63, loss = 0.00150723\n",
      "Iteration 64, loss = 0.00148552\n",
      "Iteration 65, loss = 0.00146667\n",
      "Iteration 66, loss = 0.00144112\n",
      "Iteration 67, loss = 0.00142144\n",
      "Iteration 68, loss = 0.00140007\n",
      "Iteration 69, loss = 0.00138107\n",
      "Iteration 70, loss = 0.00136299\n",
      "Iteration 71, loss = 0.00134925\n",
      "Iteration 72, loss = 0.00133294\n",
      "Iteration 73, loss = 0.00131390\n",
      "Iteration 74, loss = 0.00129716\n",
      "Iteration 75, loss = 0.00128197\n",
      "Iteration 76, loss = 0.00126801\n",
      "Iteration 77, loss = 0.00125435\n",
      "Iteration 78, loss = 0.00124177\n",
      "Iteration 79, loss = 0.00122591\n",
      "Iteration 80, loss = 0.00121361\n",
      "Iteration 81, loss = 0.00120096\n",
      "Iteration 82, loss = 0.00119060\n",
      "Iteration 83, loss = 0.00118042\n",
      "Iteration 84, loss = 0.00116968\n",
      "Iteration 85, loss = 0.00115677\n",
      "Iteration 86, loss = 0.00114762\n",
      "Iteration 87, loss = 0.00113785\n",
      "Iteration 88, loss = 0.00112927\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Training set score WITHOUT pca: 1.000000\n",
      "Test set score WITHOUT pca: 0.975857\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(\"Training set score WITHOUT pca: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score WITHOUT pca: %f\" % mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47597010\n",
      "Iteration 2, loss = 0.27553808\n",
      "Iteration 3, loss = 0.20661237\n",
      "Iteration 4, loss = 0.17570460\n",
      "Iteration 5, loss = 0.15698907\n",
      "Iteration 6, loss = 0.14412227\n",
      "Iteration 7, loss = 0.13595119\n",
      "Iteration 8, loss = 0.12803037\n",
      "Iteration 9, loss = 0.12246642\n",
      "Iteration 10, loss = 0.11801112\n",
      "Iteration 11, loss = 0.11435518\n",
      "Iteration 12, loss = 0.11102220\n",
      "Iteration 13, loss = 0.10858527\n",
      "Iteration 14, loss = 0.10565898\n",
      "Iteration 15, loss = 0.10327723\n",
      "Iteration 16, loss = 0.10111979\n",
      "Iteration 17, loss = 0.09956910\n",
      "Iteration 18, loss = 0.09747862\n",
      "Iteration 19, loss = 0.09617378\n",
      "Iteration 20, loss = 0.09509492\n",
      "Iteration 21, loss = 0.09355943\n",
      "Iteration 22, loss = 0.09261489\n",
      "Iteration 23, loss = 0.09070667\n",
      "Iteration 24, loss = 0.08952057\n",
      "Iteration 25, loss = 0.08870579\n",
      "Iteration 26, loss = 0.08712377\n",
      "Iteration 27, loss = 0.08702038\n",
      "Iteration 28, loss = 0.08586066\n",
      "Iteration 29, loss = 0.08389772\n",
      "Iteration 30, loss = 0.08364045\n",
      "Iteration 31, loss = 0.08281232\n",
      "Iteration 32, loss = 0.08306112\n",
      "Iteration 33, loss = 0.08195157\n",
      "Iteration 34, loss = 0.08039129\n",
      "Iteration 35, loss = 0.08108718\n",
      "Iteration 36, loss = 0.07963252\n",
      "Iteration 37, loss = 0.08010862\n",
      "Iteration 38, loss = 0.07913303\n",
      "Iteration 39, loss = 0.07841840\n",
      "Iteration 40, loss = 0.07778887\n",
      "Iteration 41, loss = 0.07757102\n",
      "Iteration 42, loss = 0.07706474\n",
      "Iteration 43, loss = 0.07675537\n",
      "Iteration 44, loss = 0.07673327\n",
      "Iteration 45, loss = 0.07607669\n",
      "Iteration 46, loss = 0.07616172\n",
      "Iteration 47, loss = 0.07566700\n",
      "Iteration 48, loss = 0.07575180\n",
      "Iteration 49, loss = 0.07486562\n",
      "Iteration 50, loss = 0.07419888\n",
      "Iteration 51, loss = 0.07464457\n",
      "Iteration 52, loss = 0.07352910\n",
      "Iteration 53, loss = 0.07395968\n",
      "Iteration 54, loss = 0.07402467\n",
      "Iteration 55, loss = 0.07336332\n",
      "Iteration 56, loss = 0.07334836\n",
      "Iteration 57, loss = 0.07281481\n",
      "Iteration 58, loss = 0.07309997\n",
      "Iteration 59, loss = 0.07249215\n",
      "Iteration 60, loss = 0.07246577\n",
      "Iteration 61, loss = 0.07228987\n",
      "Iteration 62, loss = 0.07132960\n",
      "Iteration 63, loss = 0.07182219\n",
      "Iteration 64, loss = 0.07156308\n",
      "Iteration 65, loss = 0.07135960\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Training set score WITH pca: 0.980036\n",
      "Test set score WITH pca: 0.958714\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train_pca, y_train)\n",
    "y_pred_pca = mlp.predict(X_test_pca)\n",
    "print(\"Training set score WITH pca: %f\" % mlp.score(X_train_pca, y_train))\n",
    "print(\"Test set score WITH pca: %f\" % mlp.score(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report without using pca\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98      1403\n",
      "          1       0.99      0.99      0.99      1574\n",
      "          2       0.96      0.98      0.97      1361\n",
      "          3       0.98      0.96      0.97      1454\n",
      "          4       0.98      0.98      0.98      1326\n",
      "          5       0.97      0.97      0.97      1255\n",
      "          6       0.98      0.99      0.98      1371\n",
      "          7       0.98      0.98      0.98      1464\n",
      "          8       0.96      0.96      0.96      1351\n",
      "          9       0.97      0.97      0.97      1441\n",
      "\n",
      "avg / total       0.98      0.98      0.98     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report without using pca')\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report using pca\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.98      1403\n",
      "          1       0.98      0.99      0.99      1574\n",
      "          2       0.95      0.96      0.96      1361\n",
      "          3       0.96      0.93      0.94      1454\n",
      "          4       0.96      0.96      0.96      1326\n",
      "          5       0.96      0.95      0.95      1255\n",
      "          6       0.97      0.98      0.97      1371\n",
      "          7       0.96      0.96      0.96      1464\n",
      "          8       0.94      0.94      0.94      1351\n",
      "          9       0.93      0.94      0.93      1441\n",
      "\n",
      "avg / total       0.96      0.96      0.96     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report using pca')\n",
    "print(classification_report(y_test, y_pred_pca, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação cruzada do treinamento sem a utilização do PCA\n",
      "Iteration 1, loss = 0.36295318\n",
      "Iteration 2, loss = 0.18725090\n",
      "Iteration 3, loss = 0.13712885\n",
      "Iteration 4, loss = 0.10735307\n",
      "Iteration 5, loss = 0.08880382\n",
      "Iteration 6, loss = 0.07407731\n",
      "Iteration 7, loss = 0.06289353\n",
      "Iteration 8, loss = 0.05323720\n",
      "Iteration 9, loss = 0.04592737\n",
      "Iteration 10, loss = 0.03943217\n",
      "Iteration 11, loss = 0.03429402\n",
      "Iteration 12, loss = 0.02925407\n",
      "Iteration 13, loss = 0.02551846\n",
      "Iteration 14, loss = 0.02186453\n",
      "Iteration 15, loss = 0.01939984\n",
      "Iteration 16, loss = 0.01725456\n",
      "Iteration 17, loss = 0.01505259\n",
      "Iteration 18, loss = 0.01322494\n",
      "Iteration 19, loss = 0.01167618\n",
      "Iteration 20, loss = 0.01060813\n",
      "Iteration 21, loss = 0.00955805\n",
      "Iteration 22, loss = 0.00857096\n",
      "Iteration 23, loss = 0.00789553\n",
      "Iteration 24, loss = 0.00719844\n",
      "Iteration 25, loss = 0.00660903\n",
      "Iteration 26, loss = 0.00624172\n",
      "Iteration 27, loss = 0.00572678\n",
      "Iteration 28, loss = 0.00539681\n",
      "Iteration 29, loss = 0.00497422\n",
      "Iteration 30, loss = 0.00472810\n",
      "Iteration 31, loss = 0.00447426\n",
      "Iteration 32, loss = 0.00423796\n",
      "Iteration 33, loss = 0.00408213\n",
      "Iteration 34, loss = 0.00385790\n",
      "Iteration 35, loss = 0.00370197\n",
      "Iteration 36, loss = 0.00351871\n",
      "Iteration 37, loss = 0.00336659\n",
      "Iteration 38, loss = 0.00327755\n",
      "Iteration 39, loss = 0.00312423\n",
      "Iteration 40, loss = 0.00301830\n",
      "Iteration 41, loss = 0.00292305\n",
      "Iteration 42, loss = 0.00281514\n",
      "Iteration 43, loss = 0.00271593\n",
      "Iteration 44, loss = 0.00264242\n",
      "Iteration 45, loss = 0.00256925\n",
      "Iteration 46, loss = 0.00248925\n",
      "Iteration 47, loss = 0.00242348\n",
      "Iteration 48, loss = 0.00235815\n",
      "Iteration 49, loss = 0.00229343\n",
      "Iteration 50, loss = 0.00224353\n",
      "Iteration 51, loss = 0.00218627\n",
      "Iteration 52, loss = 0.00213725\n",
      "Iteration 53, loss = 0.00210034\n",
      "Iteration 54, loss = 0.00204519\n",
      "Iteration 55, loss = 0.00199724\n",
      "Iteration 56, loss = 0.00195624\n",
      "Iteration 57, loss = 0.00191540\n",
      "Iteration 58, loss = 0.00187959\n",
      "Iteration 59, loss = 0.00184199\n",
      "Iteration 60, loss = 0.00180804\n",
      "Iteration 61, loss = 0.00177195\n",
      "Iteration 62, loss = 0.00174554\n",
      "Iteration 63, loss = 0.00170792\n",
      "Iteration 64, loss = 0.00167883\n",
      "Iteration 65, loss = 0.00165708\n",
      "Iteration 66, loss = 0.00162838\n",
      "Iteration 67, loss = 0.00160354\n",
      "Iteration 68, loss = 0.00157967\n",
      "Iteration 69, loss = 0.00155745\n",
      "Iteration 70, loss = 0.00152989\n",
      "Iteration 71, loss = 0.00150880\n",
      "Iteration 72, loss = 0.00148839\n",
      "Iteration 73, loss = 0.00146594\n",
      "Iteration 74, loss = 0.00144992\n",
      "Iteration 75, loss = 0.00142824\n",
      "Iteration 76, loss = 0.00141153\n",
      "Iteration 77, loss = 0.00139423\n",
      "Iteration 78, loss = 0.00137645\n",
      "Iteration 79, loss = 0.00135865\n",
      "Iteration 80, loss = 0.00134177\n",
      "Iteration 81, loss = 0.00132608\n",
      "Iteration 82, loss = 0.00131246\n",
      "Iteration 83, loss = 0.00129589\n",
      "Iteration 84, loss = 0.00128280\n",
      "Iteration 85, loss = 0.00126912\n",
      "Iteration 86, loss = 0.00125662\n",
      "Iteration 87, loss = 0.00124264\n",
      "Iteration 88, loss = 0.00123134\n",
      "Iteration 89, loss = 0.00121883\n",
      "Iteration 90, loss = 0.00120510\n",
      "Iteration 91, loss = 0.00119430\n",
      "Iteration 92, loss = 0.00118331\n",
      "Iteration 93, loss = 0.00117201\n",
      "Iteration 94, loss = 0.00116266\n",
      "Iteration 95, loss = 0.00115115\n",
      "Iteration 96, loss = 0.00113846\n",
      "Iteration 97, loss = 0.00113019\n",
      "Iteration 98, loss = 0.00112002\n",
      "Iteration 99, loss = 0.00110940\n",
      "Iteration 100, loss = 0.00110211\n",
      "Iteration 101, loss = 0.00109297\n",
      "Iteration 102, loss = 0.00108431\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36128525\n",
      "Iteration 2, loss = 0.18343861\n",
      "Iteration 3, loss = 0.13426710\n",
      "Iteration 4, loss = 0.10591755\n",
      "Iteration 5, loss = 0.08681583\n",
      "Iteration 6, loss = 0.07186369\n",
      "Iteration 7, loss = 0.06064923\n",
      "Iteration 8, loss = 0.05118624\n",
      "Iteration 9, loss = 0.04346993\n",
      "Iteration 10, loss = 0.03671588\n",
      "Iteration 11, loss = 0.03166731\n",
      "Iteration 12, loss = 0.02662508\n",
      "Iteration 13, loss = 0.02363752\n",
      "Iteration 14, loss = 0.02032521\n",
      "Iteration 15, loss = 0.01771308\n",
      "Iteration 16, loss = 0.01540502\n",
      "Iteration 17, loss = 0.01371870\n",
      "Iteration 18, loss = 0.01208793\n",
      "Iteration 19, loss = 0.01074735\n",
      "Iteration 20, loss = 0.00959414\n",
      "Iteration 21, loss = 0.00867054\n",
      "Iteration 22, loss = 0.00788698\n",
      "Iteration 23, loss = 0.00718664\n",
      "Iteration 24, loss = 0.00654074\n",
      "Iteration 25, loss = 0.00607561\n",
      "Iteration 26, loss = 0.00569668\n",
      "Iteration 27, loss = 0.00531339\n",
      "Iteration 28, loss = 0.00496804\n",
      "Iteration 29, loss = 0.00469407\n",
      "Iteration 30, loss = 0.00447605\n",
      "Iteration 31, loss = 0.00421949\n",
      "Iteration 32, loss = 0.00399904\n",
      "Iteration 33, loss = 0.00380009\n",
      "Iteration 34, loss = 0.00364856\n",
      "Iteration 35, loss = 0.00349832\n",
      "Iteration 36, loss = 0.00336252\n",
      "Iteration 37, loss = 0.00321083\n",
      "Iteration 38, loss = 0.00309771\n",
      "Iteration 39, loss = 0.00298252\n",
      "Iteration 40, loss = 0.00287651\n",
      "Iteration 41, loss = 0.00277156\n",
      "Iteration 42, loss = 0.00269433\n",
      "Iteration 43, loss = 0.00260842\n",
      "Iteration 44, loss = 0.00253055\n",
      "Iteration 45, loss = 0.00246059\n",
      "Iteration 46, loss = 0.00238786\n",
      "Iteration 47, loss = 0.00232884\n",
      "Iteration 48, loss = 0.00226412\n",
      "Iteration 49, loss = 0.00221385\n",
      "Iteration 50, loss = 0.00214859\n",
      "Iteration 51, loss = 0.00210647\n",
      "Iteration 52, loss = 0.00205607\n",
      "Iteration 53, loss = 0.00200767\n",
      "Iteration 54, loss = 0.00196707\n",
      "Iteration 55, loss = 0.00192063\n",
      "Iteration 56, loss = 0.00187796\n",
      "Iteration 57, loss = 0.00184566\n",
      "Iteration 58, loss = 0.00181150\n",
      "Iteration 59, loss = 0.00177070\n",
      "Iteration 60, loss = 0.00174437\n",
      "Iteration 61, loss = 0.00171246\n",
      "Iteration 62, loss = 0.00167956\n",
      "Iteration 63, loss = 0.00165119\n",
      "Iteration 64, loss = 0.00162568\n",
      "Iteration 65, loss = 0.00159609\n",
      "Iteration 66, loss = 0.00157528\n",
      "Iteration 67, loss = 0.00154641\n",
      "Iteration 68, loss = 0.00152429\n",
      "Iteration 69, loss = 0.00150139\n",
      "Iteration 70, loss = 0.00147842\n",
      "Iteration 71, loss = 0.00146249\n",
      "Iteration 72, loss = 0.00143790\n",
      "Iteration 73, loss = 0.00141679\n",
      "Iteration 74, loss = 0.00139886\n",
      "Iteration 75, loss = 0.00138125\n",
      "Iteration 76, loss = 0.00136580\n",
      "Iteration 77, loss = 0.00135006\n",
      "Iteration 78, loss = 0.00133141\n",
      "Iteration 79, loss = 0.00131371\n",
      "Iteration 80, loss = 0.00130072\n",
      "Iteration 81, loss = 0.00128215\n",
      "Iteration 82, loss = 0.00126940\n",
      "Iteration 83, loss = 0.00125549\n",
      "Iteration 84, loss = 0.00124272\n",
      "Iteration 85, loss = 0.00122583\n",
      "Iteration 86, loss = 0.00121466\n",
      "Iteration 87, loss = 0.00120368\n",
      "Iteration 88, loss = 0.00119098\n",
      "Iteration 89, loss = 0.00117920\n",
      "Iteration 90, loss = 0.00116726\n",
      "Iteration 91, loss = 0.00115741\n",
      "Iteration 92, loss = 0.00114393\n",
      "Iteration 93, loss = 0.00113362\n",
      "Iteration 94, loss = 0.00112374\n",
      "Iteration 95, loss = 0.00111610\n",
      "Iteration 96, loss = 0.00110462\n",
      "Iteration 97, loss = 0.00109603\n",
      "Iteration 98, loss = 0.00108648\n",
      "Iteration 99, loss = 0.00107738\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35780066\n",
      "Iteration 2, loss = 0.18061803\n",
      "Iteration 3, loss = 0.13336030\n",
      "Iteration 4, loss = 0.10549258\n",
      "Iteration 5, loss = 0.08673164\n",
      "Iteration 6, loss = 0.07207143\n",
      "Iteration 7, loss = 0.05967953\n",
      "Iteration 8, loss = 0.05181613\n",
      "Iteration 9, loss = 0.04377719\n",
      "Iteration 10, loss = 0.03698023\n",
      "Iteration 11, loss = 0.03174469\n",
      "Iteration 12, loss = 0.02738699\n",
      "Iteration 13, loss = 0.02388163\n",
      "Iteration 14, loss = 0.02016107\n",
      "Iteration 15, loss = 0.01771590\n",
      "Iteration 16, loss = 0.01552807\n",
      "Iteration 17, loss = 0.01353434\n",
      "Iteration 18, loss = 0.01200362\n",
      "Iteration 19, loss = 0.01060025\n",
      "Iteration 20, loss = 0.00955041\n",
      "Iteration 21, loss = 0.00855480\n",
      "Iteration 22, loss = 0.00787099\n",
      "Iteration 23, loss = 0.00709985\n",
      "Iteration 24, loss = 0.00664545\n",
      "Iteration 25, loss = 0.00606406\n",
      "Iteration 26, loss = 0.00567193\n",
      "Iteration 27, loss = 0.00530114\n",
      "Iteration 28, loss = 0.00495365\n",
      "Iteration 29, loss = 0.00471555\n",
      "Iteration 30, loss = 0.00441569\n",
      "Iteration 31, loss = 0.00416676\n",
      "Iteration 32, loss = 0.00397835\n",
      "Iteration 33, loss = 0.00381848\n",
      "Iteration 34, loss = 0.00362758\n",
      "Iteration 35, loss = 0.00347950\n",
      "Iteration 36, loss = 0.00331996\n",
      "Iteration 37, loss = 0.00319403\n",
      "Iteration 38, loss = 0.00308654\n",
      "Iteration 39, loss = 0.00295621\n",
      "Iteration 40, loss = 0.00287371\n",
      "Iteration 41, loss = 0.00278151\n",
      "Iteration 42, loss = 0.00266890\n",
      "Iteration 43, loss = 0.00261033\n",
      "Iteration 44, loss = 0.00250446\n",
      "Iteration 45, loss = 0.00244380\n",
      "Iteration 46, loss = 0.00237958\n",
      "Iteration 47, loss = 0.00231033\n",
      "Iteration 48, loss = 0.00226423\n",
      "Iteration 49, loss = 0.00219622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.00213771\n",
      "Iteration 51, loss = 0.00208917\n",
      "Iteration 52, loss = 0.00204032\n",
      "Iteration 53, loss = 0.00200173\n",
      "Iteration 54, loss = 0.00196072\n",
      "Iteration 55, loss = 0.00191376\n",
      "Iteration 56, loss = 0.00187170\n",
      "Iteration 57, loss = 0.00184737\n",
      "Iteration 58, loss = 0.00180108\n",
      "Iteration 59, loss = 0.00177229\n",
      "Iteration 60, loss = 0.00173929\n",
      "Iteration 61, loss = 0.00170386\n",
      "Iteration 62, loss = 0.00167811\n",
      "Iteration 63, loss = 0.00164815\n",
      "Iteration 64, loss = 0.00161944\n",
      "Iteration 65, loss = 0.00159057\n",
      "Iteration 66, loss = 0.00156545\n",
      "Iteration 67, loss = 0.00154443\n",
      "Iteration 68, loss = 0.00152200\n",
      "Iteration 69, loss = 0.00149521\n",
      "Iteration 70, loss = 0.00147625\n",
      "Iteration 71, loss = 0.00145930\n",
      "Iteration 72, loss = 0.00143764\n",
      "Iteration 73, loss = 0.00141622\n",
      "Iteration 74, loss = 0.00139568\n",
      "Iteration 75, loss = 0.00137882\n",
      "Iteration 76, loss = 0.00135943\n",
      "Iteration 77, loss = 0.00134468\n",
      "Iteration 78, loss = 0.00132575\n",
      "Iteration 79, loss = 0.00131290\n",
      "Iteration 80, loss = 0.00129581\n",
      "Iteration 81, loss = 0.00128052\n",
      "Iteration 82, loss = 0.00126624\n",
      "Iteration 83, loss = 0.00125041\n",
      "Iteration 84, loss = 0.00124178\n",
      "Iteration 85, loss = 0.00122366\n",
      "Iteration 86, loss = 0.00121295\n",
      "Iteration 87, loss = 0.00119896\n",
      "Iteration 88, loss = 0.00118814\n",
      "Iteration 89, loss = 0.00117808\n",
      "Iteration 90, loss = 0.00116546\n",
      "Iteration 91, loss = 0.00115273\n",
      "Iteration 92, loss = 0.00114584\n",
      "Iteration 93, loss = 0.00113297\n",
      "Iteration 94, loss = 0.00112248\n",
      "Iteration 95, loss = 0.00111285\n",
      "Iteration 96, loss = 0.00110364\n",
      "Iteration 97, loss = 0.00109434\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97230352422234478"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Validação cruzada do treinamento sem a utilização do PCA')\n",
    "scores = cross_val_score(mlp,X_train,y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97585714285714287"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação cruzada do treinamento utilizando PCA para fazer o redimencionamento das imagens\n",
      "Iteration 1, loss = 0.52776568\n",
      "Iteration 2, loss = 0.33775235\n",
      "Iteration 3, loss = 0.25645380\n",
      "Iteration 4, loss = 0.21202017\n",
      "Iteration 5, loss = 0.18559166\n",
      "Iteration 6, loss = 0.16782606\n",
      "Iteration 7, loss = 0.15566921\n",
      "Iteration 8, loss = 0.14525291\n",
      "Iteration 9, loss = 0.13775549\n",
      "Iteration 10, loss = 0.13123760\n",
      "Iteration 11, loss = 0.12489411\n",
      "Iteration 12, loss = 0.12059306\n",
      "Iteration 13, loss = 0.11659855\n",
      "Iteration 14, loss = 0.11407488\n",
      "Iteration 15, loss = 0.11033524\n",
      "Iteration 16, loss = 0.10772699\n",
      "Iteration 17, loss = 0.10404612\n",
      "Iteration 18, loss = 0.10135042\n",
      "Iteration 19, loss = 0.09945791\n",
      "Iteration 20, loss = 0.09677847\n",
      "Iteration 21, loss = 0.09511981\n",
      "Iteration 22, loss = 0.09384707\n",
      "Iteration 23, loss = 0.09181707\n",
      "Iteration 24, loss = 0.09014078\n",
      "Iteration 25, loss = 0.08881886\n",
      "Iteration 26, loss = 0.08734575\n",
      "Iteration 27, loss = 0.08512577\n",
      "Iteration 28, loss = 0.08467688\n",
      "Iteration 29, loss = 0.08276047\n",
      "Iteration 30, loss = 0.08228798\n",
      "Iteration 31, loss = 0.08076686\n",
      "Iteration 32, loss = 0.07988104\n",
      "Iteration 33, loss = 0.07880243\n",
      "Iteration 34, loss = 0.07763887\n",
      "Iteration 35, loss = 0.07664744\n",
      "Iteration 36, loss = 0.07627550\n",
      "Iteration 37, loss = 0.07533311\n",
      "Iteration 38, loss = 0.07417224\n",
      "Iteration 39, loss = 0.07317433\n",
      "Iteration 40, loss = 0.07249408\n",
      "Iteration 41, loss = 0.07197111\n",
      "Iteration 42, loss = 0.07185138\n",
      "Iteration 43, loss = 0.07167160\n",
      "Iteration 44, loss = 0.06991171\n",
      "Iteration 45, loss = 0.06973984\n",
      "Iteration 46, loss = 0.06907929\n",
      "Iteration 47, loss = 0.06852273\n",
      "Iteration 48, loss = 0.06854271\n",
      "Iteration 49, loss = 0.06760475\n",
      "Iteration 50, loss = 0.06764135\n",
      "Iteration 51, loss = 0.06720274\n",
      "Iteration 52, loss = 0.06654116\n",
      "Iteration 53, loss = 0.06644623\n",
      "Iteration 54, loss = 0.06499698\n",
      "Iteration 55, loss = 0.06526637\n",
      "Iteration 56, loss = 0.06486096\n",
      "Iteration 57, loss = 0.06389602\n",
      "Iteration 58, loss = 0.06393133\n",
      "Iteration 59, loss = 0.06418318\n",
      "Iteration 60, loss = 0.06367871\n",
      "Iteration 61, loss = 0.06244334\n",
      "Iteration 62, loss = 0.06276641\n",
      "Iteration 63, loss = 0.06228096\n",
      "Iteration 64, loss = 0.06191428\n",
      "Iteration 65, loss = 0.06141333\n",
      "Iteration 66, loss = 0.06159486\n",
      "Iteration 67, loss = 0.06100709\n",
      "Iteration 68, loss = 0.06016813\n",
      "Iteration 69, loss = 0.06044817\n",
      "Iteration 70, loss = 0.06014668\n",
      "Iteration 71, loss = 0.05922312\n",
      "Iteration 72, loss = 0.05901815\n",
      "Iteration 73, loss = 0.05899862\n",
      "Iteration 74, loss = 0.05835072\n",
      "Iteration 75, loss = 0.05822226\n",
      "Iteration 76, loss = 0.05809006\n",
      "Iteration 77, loss = 0.05743533\n",
      "Iteration 78, loss = 0.05739935\n",
      "Iteration 79, loss = 0.05724623\n",
      "Iteration 80, loss = 0.05683705\n",
      "Iteration 81, loss = 0.05660556\n",
      "Iteration 82, loss = 0.05620738\n",
      "Iteration 83, loss = 0.05579762\n",
      "Iteration 84, loss = 0.05571012\n",
      "Iteration 85, loss = 0.05560585\n",
      "Iteration 86, loss = 0.05532491\n",
      "Iteration 87, loss = 0.05535950\n",
      "Iteration 88, loss = 0.05498651\n",
      "Iteration 89, loss = 0.05521707\n",
      "Iteration 90, loss = 0.05404242\n",
      "Iteration 91, loss = 0.05446757\n",
      "Iteration 92, loss = 0.05400567\n",
      "Iteration 93, loss = 0.05393673\n",
      "Iteration 94, loss = 0.05381012\n",
      "Iteration 95, loss = 0.05381634\n",
      "Iteration 96, loss = 0.05258443\n",
      "Iteration 97, loss = 0.05284082\n",
      "Iteration 98, loss = 0.05305906\n",
      "Iteration 99, loss = 0.05251494\n",
      "Iteration 100, loss = 0.05233456\n",
      "Iteration 101, loss = 0.05175248\n",
      "Iteration 102, loss = 0.05215203\n",
      "Iteration 103, loss = 0.05223942\n",
      "Iteration 104, loss = 0.05160019\n",
      "Iteration 105, loss = 0.05246445\n",
      "Iteration 106, loss = 0.05174976\n",
      "Iteration 107, loss = 0.05114093\n",
      "Iteration 108, loss = 0.05057119\n",
      "Iteration 109, loss = 0.05117855\n",
      "Iteration 110, loss = 0.05051412\n",
      "Iteration 111, loss = 0.05035691\n",
      "Iteration 112, loss = 0.04984867\n",
      "Iteration 113, loss = 0.05063330\n",
      "Iteration 114, loss = 0.04935320\n",
      "Iteration 115, loss = 0.04926235\n",
      "Iteration 116, loss = 0.04923395\n",
      "Iteration 117, loss = 0.04894366\n",
      "Iteration 118, loss = 0.04951523\n",
      "Iteration 119, loss = 0.04927097\n",
      "Iteration 120, loss = 0.04870288\n",
      "Iteration 121, loss = 0.04852260\n",
      "Iteration 122, loss = 0.04891253\n",
      "Iteration 123, loss = 0.04825611\n",
      "Iteration 124, loss = 0.04819572\n",
      "Iteration 125, loss = 0.04816228\n",
      "Iteration 126, loss = 0.04845323\n",
      "Iteration 127, loss = 0.04774566\n",
      "Iteration 128, loss = 0.04832927\n",
      "Iteration 129, loss = 0.04741328\n",
      "Iteration 130, loss = 0.04716386\n",
      "Iteration 131, loss = 0.04781602\n",
      "Iteration 132, loss = 0.04693715\n",
      "Iteration 133, loss = 0.04686822\n",
      "Iteration 134, loss = 0.04739479\n",
      "Iteration 135, loss = 0.04646954\n",
      "Iteration 136, loss = 0.04652574\n",
      "Iteration 137, loss = 0.04632628\n",
      "Iteration 138, loss = 0.04640388\n",
      "Iteration 139, loss = 0.04585322\n",
      "Iteration 140, loss = 0.04608675\n",
      "Iteration 141, loss = 0.04621276\n",
      "Iteration 142, loss = 0.04619916\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52929975\n",
      "Iteration 2, loss = 0.33442930\n",
      "Iteration 3, loss = 0.25258952\n",
      "Iteration 4, loss = 0.20951386\n",
      "Iteration 5, loss = 0.18537932\n",
      "Iteration 6, loss = 0.16720851\n",
      "Iteration 7, loss = 0.15557964\n",
      "Iteration 8, loss = 0.14663720\n",
      "Iteration 9, loss = 0.13919734\n",
      "Iteration 10, loss = 0.13209628\n",
      "Iteration 11, loss = 0.12624105\n",
      "Iteration 12, loss = 0.12183240\n",
      "Iteration 13, loss = 0.11775347\n",
      "Iteration 14, loss = 0.11443455\n",
      "Iteration 15, loss = 0.11043990\n",
      "Iteration 16, loss = 0.10751830\n",
      "Iteration 17, loss = 0.10500831\n",
      "Iteration 18, loss = 0.10183158\n",
      "Iteration 19, loss = 0.09948819\n",
      "Iteration 20, loss = 0.09813930\n",
      "Iteration 21, loss = 0.09559976\n",
      "Iteration 22, loss = 0.09463968\n",
      "Iteration 23, loss = 0.09228713\n",
      "Iteration 24, loss = 0.09113707\n",
      "Iteration 25, loss = 0.09012205\n",
      "Iteration 26, loss = 0.08849890\n",
      "Iteration 27, loss = 0.08698098\n",
      "Iteration 28, loss = 0.08623022\n",
      "Iteration 29, loss = 0.08407918\n",
      "Iteration 30, loss = 0.08365124\n",
      "Iteration 31, loss = 0.08167215\n",
      "Iteration 32, loss = 0.08153371\n",
      "Iteration 33, loss = 0.08042722\n",
      "Iteration 34, loss = 0.07983708\n",
      "Iteration 35, loss = 0.07881239\n",
      "Iteration 36, loss = 0.07741981\n",
      "Iteration 37, loss = 0.07724514\n",
      "Iteration 38, loss = 0.07630228\n",
      "Iteration 39, loss = 0.07578337\n",
      "Iteration 40, loss = 0.07449321\n",
      "Iteration 41, loss = 0.07432126\n",
      "Iteration 42, loss = 0.07345031\n",
      "Iteration 43, loss = 0.07301750\n",
      "Iteration 44, loss = 0.07171690\n",
      "Iteration 45, loss = 0.07145870\n",
      "Iteration 46, loss = 0.07127919\n",
      "Iteration 47, loss = 0.07006345\n",
      "Iteration 48, loss = 0.06904600\n",
      "Iteration 49, loss = 0.06897226\n",
      "Iteration 50, loss = 0.06840400\n",
      "Iteration 51, loss = 0.06813392\n",
      "Iteration 52, loss = 0.06710760\n",
      "Iteration 53, loss = 0.06668814\n",
      "Iteration 54, loss = 0.06669669\n",
      "Iteration 55, loss = 0.06609400\n",
      "Iteration 56, loss = 0.06569139\n",
      "Iteration 57, loss = 0.06450634\n",
      "Iteration 58, loss = 0.06496885\n",
      "Iteration 59, loss = 0.06424255\n",
      "Iteration 60, loss = 0.06430546\n",
      "Iteration 61, loss = 0.06375490\n",
      "Iteration 62, loss = 0.06299787\n",
      "Iteration 63, loss = 0.06289598\n",
      "Iteration 64, loss = 0.06221168\n",
      "Iteration 65, loss = 0.06196446\n",
      "Iteration 66, loss = 0.06214216\n",
      "Iteration 67, loss = 0.06143442\n",
      "Iteration 68, loss = 0.06093881\n",
      "Iteration 69, loss = 0.06024042\n",
      "Iteration 70, loss = 0.06025731\n",
      "Iteration 71, loss = 0.05977575\n",
      "Iteration 72, loss = 0.05961906\n",
      "Iteration 73, loss = 0.05987557\n",
      "Iteration 74, loss = 0.05922683\n",
      "Iteration 75, loss = 0.05913273\n",
      "Iteration 76, loss = 0.05887692\n",
      "Iteration 77, loss = 0.05777616\n",
      "Iteration 78, loss = 0.05786350\n",
      "Iteration 79, loss = 0.05765830\n",
      "Iteration 80, loss = 0.05802535\n",
      "Iteration 81, loss = 0.05674158\n",
      "Iteration 82, loss = 0.05695655\n",
      "Iteration 83, loss = 0.05694518\n",
      "Iteration 84, loss = 0.05709511\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52752432\n",
      "Iteration 2, loss = 0.33517423\n",
      "Iteration 3, loss = 0.25334822\n",
      "Iteration 4, loss = 0.21025323\n",
      "Iteration 5, loss = 0.18568014\n",
      "Iteration 6, loss = 0.16813310\n",
      "Iteration 7, loss = 0.15494459\n",
      "Iteration 8, loss = 0.14514776\n",
      "Iteration 9, loss = 0.13762106\n",
      "Iteration 10, loss = 0.13119926\n",
      "Iteration 11, loss = 0.12620743\n",
      "Iteration 12, loss = 0.12049079\n",
      "Iteration 13, loss = 0.11718357\n",
      "Iteration 14, loss = 0.11305661\n",
      "Iteration 15, loss = 0.11001912\n",
      "Iteration 16, loss = 0.10694718\n",
      "Iteration 17, loss = 0.10345627\n",
      "Iteration 18, loss = 0.10113996\n",
      "Iteration 19, loss = 0.09998665\n",
      "Iteration 20, loss = 0.09636968\n",
      "Iteration 21, loss = 0.09503447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.09332596\n",
      "Iteration 23, loss = 0.09143669\n",
      "Iteration 24, loss = 0.08936949\n",
      "Iteration 25, loss = 0.08741458\n",
      "Iteration 26, loss = 0.08651237\n",
      "Iteration 27, loss = 0.08544186\n",
      "Iteration 28, loss = 0.08344568\n",
      "Iteration 29, loss = 0.08292403\n",
      "Iteration 30, loss = 0.08104038\n",
      "Iteration 31, loss = 0.08087515\n",
      "Iteration 32, loss = 0.07942416\n",
      "Iteration 33, loss = 0.07898207\n",
      "Iteration 34, loss = 0.07791143\n",
      "Iteration 35, loss = 0.07669972\n",
      "Iteration 36, loss = 0.07604529\n",
      "Iteration 37, loss = 0.07537275\n",
      "Iteration 38, loss = 0.07425266\n",
      "Iteration 39, loss = 0.07325537\n",
      "Iteration 40, loss = 0.07278021\n",
      "Iteration 41, loss = 0.07207353\n",
      "Iteration 42, loss = 0.07153561\n",
      "Iteration 43, loss = 0.07032840\n",
      "Iteration 44, loss = 0.07018182\n",
      "Iteration 45, loss = 0.06922500\n",
      "Iteration 46, loss = 0.06875317\n",
      "Iteration 47, loss = 0.06833862\n",
      "Iteration 48, loss = 0.06814342\n",
      "Iteration 49, loss = 0.06713954\n",
      "Iteration 50, loss = 0.06646555\n",
      "Iteration 51, loss = 0.06624222\n",
      "Iteration 52, loss = 0.06605816\n",
      "Iteration 53, loss = 0.06548428\n",
      "Iteration 54, loss = 0.06445748\n",
      "Iteration 55, loss = 0.06445061\n",
      "Iteration 56, loss = 0.06385228\n",
      "Iteration 57, loss = 0.06332542\n",
      "Iteration 58, loss = 0.06315581\n",
      "Iteration 59, loss = 0.06273792\n",
      "Iteration 60, loss = 0.06263250\n",
      "Iteration 61, loss = 0.06204281\n",
      "Iteration 62, loss = 0.06204735\n",
      "Iteration 63, loss = 0.06114686\n",
      "Iteration 64, loss = 0.06079327\n",
      "Iteration 65, loss = 0.06039089\n",
      "Iteration 66, loss = 0.06044092\n",
      "Iteration 67, loss = 0.05986894\n",
      "Iteration 68, loss = 0.05941417\n",
      "Iteration 69, loss = 0.05869310\n",
      "Iteration 70, loss = 0.05897724\n",
      "Iteration 71, loss = 0.05871974\n",
      "Iteration 72, loss = 0.05844424\n",
      "Iteration 73, loss = 0.05807803\n",
      "Iteration 74, loss = 0.05776720\n",
      "Iteration 75, loss = 0.05740787\n",
      "Iteration 76, loss = 0.05752370\n",
      "Iteration 77, loss = 0.05688549\n",
      "Iteration 78, loss = 0.05694903\n",
      "Iteration 79, loss = 0.05667932\n",
      "Iteration 80, loss = 0.05628340\n",
      "Iteration 81, loss = 0.05642756\n",
      "Iteration 82, loss = 0.05626258\n",
      "Iteration 83, loss = 0.05561783\n",
      "Iteration 84, loss = 0.05485933\n",
      "Iteration 85, loss = 0.05450170\n",
      "Iteration 86, loss = 0.05516589\n",
      "Iteration 87, loss = 0.05445581\n",
      "Iteration 88, loss = 0.05428328\n",
      "Iteration 89, loss = 0.05436109\n",
      "Iteration 90, loss = 0.05390351\n",
      "Iteration 91, loss = 0.05324611\n",
      "Iteration 92, loss = 0.05336549\n",
      "Iteration 93, loss = 0.05339047\n",
      "Iteration 94, loss = 0.05236605\n",
      "Iteration 95, loss = 0.05345423\n",
      "Iteration 96, loss = 0.05257472\n",
      "Iteration 97, loss = 0.05245492\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95401794874220303"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Validação cruzada do treinamento utilizando PCA para fazer o redimencionamento das imagens')\n",
    "scores = cross_val_score(mlp,X_train_pca,y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95871428571428574"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,y_pred_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.,  7.,  7.,  5.,  3.,  1.,  1.,  1.,  9.,  5.,  3.,  5.,  6.,\n",
       "        9.,  6.,  8.,  3.,  0.,  3.,  4.,  6.,  3.,  5.,  8.,  3.,  4.,\n",
       "        1.,  3.,  6.,  2.,  3.,  1.,  9.,  3.,  8.,  3.,  7.,  0.,  5.,\n",
       "        8.,  4.,  0.,  5.,  3.,  0.,  5.,  2.,  3.,  8.,  7.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_pca[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.,  7.,  7.,  5.,  3.,  1.,  1.,  1.,  9.,  5.,  3.,  5.,  6.,\n",
       "        9.,  6.,  8.,  3.,  0.,  8.,  4.,  6.,  3.,  5.,  8.,  3.,  4.,\n",
       "        1.,  3.,  6.,  2.,  3.,  1.,  9.,  3.,  8.,  3.,  7.,  0.,  5.,\n",
       "        8.,  4.,  0.,  5.,  3.,  0.,  5.,  2.,  3.,  8.,  7.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.,  7.,  7.,  5.,  3.,  1.,  1.,  1.,  9.,  5.,  3.,  5.,  6.,\n",
       "        9.,  6.,  8.,  3.,  0.,  8.,  4.,  6.,  3.,  5.,  8.,  3.,  4.,\n",
       "        1.,  3.,  6.,  2.,  3.,  1.,  9.,  3.,  8.,  3.,  7.,  0.,  5.,\n",
       "        8.,  4.,  0.,  5.,  3.,  0.,  5.,  2.,  3.,  8.,  7.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento utilizando o modelo MLPClassifier com outros parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(89,), activation='relu', max_iter=1000, alpha=1e-4,\n",
    "                    solver='lbfgs', verbose=10, tol=1e-5, random_state=1,\n",
    "                    learning_rate_init=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score WITH pca: 0.988482\n",
      "Test set score WITH pca: 0.958571\n"
     ]
    }
   ],
   "source": [
    "mlp2.fit(X_train_pca, y_train)\n",
    "y_pred_pca2 = mlp2.predict(X_test_pca)\n",
    "print(\"Training set score WITH pca: %f\" % mlp2.score(X_train_pca, y_train))\n",
    "print(\"Test set score WITH pca: %f\" % mlp2.score(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report using pca\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      1403\n",
      "          1       0.98      0.99      0.98      1574\n",
      "          2       0.95      0.96      0.95      1361\n",
      "          3       0.96      0.93      0.94      1454\n",
      "          4       0.96      0.96      0.96      1326\n",
      "          5       0.95      0.96      0.95      1255\n",
      "          6       0.97      0.98      0.98      1371\n",
      "          7       0.96      0.95      0.96      1464\n",
      "          8       0.93      0.95      0.94      1351\n",
      "          9       0.93      0.93      0.93      1441\n",
      "\n",
      "avg / total       0.96      0.96      0.96     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report using pca')\n",
    "print(classification_report(y_test, y_pred_pca2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação cruzada do treinamento com a utilização do PCA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94712504337654424"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Validação cruzada do treinamento com a utilização do PCA')\n",
    "scores = cross_val_score(mlp2,X_train_pca,y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95857142857142852"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,y_pred_pca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
